{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roquero/miniconda3/lib/python3.7/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n"
     ]
    }
   ],
   "source": [
    "import sys, os, itertools, sklearn\n",
    "sys.path.append('/home/roquero/CausalAggregation/Code')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import auc\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(suppress=True, edgeitems=10)\n",
    "np.core.arrayprint._line_width = 80\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "from backfitting import Backfitting\n",
    "from boosting import Boosting\n",
    "from base_environment import BaseEnvironment\n",
    "from collection_environment import CollectionEnvironment\n",
    "from regression_method import PolynomialRegression, RandomForestRegression, DecisionTreeRegression\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ik_h, se_h = [], (lambda input_samples:input_samples)\n",
    "ik_x1, se_x1 = ['h'], (lambda input_samples: 2*input_samples[0] + input_samples[1])\n",
    "ik_x2, se_x2 = ['h', 'x1'], (lambda input_samples: input_samples[0] + input_samples[1] + input_samples[2])\n",
    "ik_x3, se_x3 = ['x1', 'x2'], (lambda input_samples: -input_samples[0] * 2*input_samples[1] + input_samples[2])\n",
    "ik_x4, se_x4 = ['x1', 'x3'], (lambda input_samples: np.log(1+np.abs(input_samples[0])) + input_samples[1] + input_samples[2])\n",
    "\n",
    "ik_y, se_y = ['h','x1','x2','x3','x4'], (lambda input_samples:\n",
    "                                        2*input_samples[0] \n",
    "                                         + (input_samples[1]>0)*1.\n",
    "                                        + (input_samples[2]>0)*1.\n",
    "                                         - (input_samples[2]>0)*(input_samples[3]<-1)*5.\n",
    "                                         + (input_samples[1]<0)*(input_samples[4]<-1)*2.\n",
    "                                         + (input_samples[1]<0)*(input_samples[2]<1)*(input_samples[3]<-1)*3.\n",
    "                                         + 0.5*input_samples[5])\n",
    "ik_x5, se_x5 = ['x2', 'x4', 'y'], (lambda input_samples: 2*input_samples[0] + input_samples[1] - input_samples[2] + input_samples[3])\n",
    "\n",
    "fh = {'input_keys':ik_h, 'structural_eq': se_h}\n",
    "f1 = {'input_keys':ik_x1, 'structural_eq': se_x1}\n",
    "f2 = {'input_keys':ik_x2, 'structural_eq': se_x2}\n",
    "f3 = {'input_keys':ik_x3, 'structural_eq': se_x3}\n",
    "f4 = {'input_keys':ik_x4, 'structural_eq': se_x4}\n",
    "f5 = {'input_keys':ik_x5, 'structural_eq': se_x5}\n",
    "fy = {'input_keys':ik_y, 'structural_eq': se_y}\n",
    "\n",
    "sh = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s1 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s2 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s3 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s4 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s5 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "sy = lambda n_samples: np.random.normal(size=n_samples)\n",
    "\n",
    "topo_order = ['h','x1','x2','x3','x4','y','x5']\n",
    "y_key = 'y'\n",
    "x_key = ['x1','x2','x3','x4','x5']\n",
    "\n",
    "structural_equation_dict = {'x1':f1, 'x2':f2, 'x3':f3, 'y':fy, 'x4':f4, 'x5':f5, 'h':fh}\n",
    "disturbance_sampler_dict = {'x1':s1, 'x2':s2, 'x3':s3, 'y':sy, 'x4':s4, 'x5':s5, 'h':sh}\n",
    "\n",
    "base = BaseEnvironment(structural_equation_dict, disturbance_sampler_dict, topo_order, y_key, x_key)\n",
    "coll_env = CollectionEnvironment(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.508592401324002, 2.7585330478969086, 1.922993710961037, 0.8781046988675651, 0.4672995301650899, 0.29369820696375204, 0.11086227171898078] [5.059902264489634, 1.043297179731412, 0.9326258298903386, 0.7742364009132131, 0.21117211747667244, 0.2562535358991834, 0.03417496475616383] [4.83477303507988, 4.194141744853142, 4.141058283544043, 3.9898047169430924, 3.783363897570649, 3.570072075578609, 3.4623146910693867] [1.317486196656127, 0.46781936851116523, 0.36145167916514004, 0.16371896819577872, 0.11588000286035409, 0.07773490518376432, 0.0460229475396549]\n"
     ]
    }
   ],
   "source": [
    "n_samples_list = [100,200,500,1000,2000,5000,10000]\n",
    "loss_mean_list, loss_std_list, naive_loss_mean_list, naive_loss_std_list = [], [], [], []\n",
    "\n",
    "for n_samples in n_samples_list:\n",
    "    loss_list=[]\n",
    "    naive_loss_list=[]\n",
    "    n_rep=10\n",
    "    for i in np.arange(n_rep):\n",
    "        coll_env.reset_env()\n",
    "        coll_env.add_env('e1', {'x1':{'type':'independent'},'x2':{'type':'independent'},'x3':{'type':'independent'}}, n_samples)\n",
    "        coll_env.add_env('e2', {'x1':{'type':'independent'},'x4':{'type':'independent'}}, n_samples)\n",
    "        coll_env.add_env('e3', {'x2':{'type':'independent'},'x3':{'type':'independent'}}, n_samples)\n",
    "        \n",
    "        backfit = Backfitting(DecisionTreeRegression, \n",
    "                              'boosting',\n",
    "                              max_n_iter=10,\n",
    "                              gap_convergence=1e-5,\n",
    "                              warm_start=False, \n",
    "                              params_method={},\n",
    "                              reweighting_candidates=True,\n",
    "                              update_within_loop=False\n",
    "                             )\n",
    "        try:\n",
    "            loss_list.append(backfit.fitCV(coll_env))\n",
    "            naive_loss_list.append(backfit.fit_naiveCV(coll_env))   \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    loss_mean_list.append(np.mean(np.stack(loss_list)[:,1]))\n",
    "    loss_std_list.append(np.std(np.stack(loss_list)[:,1]))\n",
    "    naive_loss_mean_list.append(np.mean(np.stack(naive_loss_list)[:,1]))\n",
    "    naive_loss_std_list.append(np.std(np.stack(naive_loss_list)[:,1]))\n",
    "print(loss_mean_list, loss_std_list, naive_loss_mean_list, naive_loss_std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples_list = [100,200,500,1000,2000,5000,10000]\n",
    "loss_mean_list, loss_std_list, naive_loss_mean_list, naive_loss_std_list = [], [], [], []\n",
    "\n",
    "for n_samples in n_samples_list:\n",
    "    loss_list=[]\n",
    "    naive_loss_list=[]\n",
    "    n_rep=10\n",
    "    for i in np.arange(n_rep):\n",
    "        coll_env.reset_env()\n",
    "        coll_env.add_env('e1', {'x1':{'type':'independent'}}, n_samples)\n",
    "        coll_env.add_env('e2', {'x1':{'type':'independent'}}, n_samples)\n",
    "        coll_env.add_env('e3', {'x2':{'type':'independent'}}, n_samples)\n",
    "        \n",
    "        backfit = Backfitting(DecisionTreeRegression, \n",
    "                              'boosting',\n",
    "                              max_n_iter=10,\n",
    "                              gap_convergence=1e-5,\n",
    "                              warm_start=False, \n",
    "                              params_method={},\n",
    "                              reweighting_candidates=True,\n",
    "                              update_within_loop=False\n",
    "                             )\n",
    "        try:\n",
    "            loss_list.append(backfit.fitCV(coll_env))\n",
    "            naive_loss_list.append(backfit.fit_naiveCV(coll_env))   \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    loss_mean_list.append(np.mean(np.stack(loss_list)[:,1]))\n",
    "    loss_std_list.append(np.std(np.stack(loss_list)[:,1]))\n",
    "    naive_loss_mean_list.append(np.mean(np.stack(naive_loss_list)[:,1]))\n",
    "    naive_loss_std_list.append(np.std(np.stack(naive_loss_list)[:,1]))\n",
    "print(loss_mean_list, loss_std_list, naive_loss_mean_list, naive_loss_std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misspecified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ik_h, se_h = [], (lambda input_samples:input_samples)\n",
    "ik_x1, se_x1 = ['h'], (lambda input_samples: 2*input_samples[0] + input_samples[1])\n",
    "ik_x2, se_x2 = ['h', 'x1'], (lambda input_samples: input_samples[0] + input_samples[1] + input_samples[2])\n",
    "ik_x3, se_x3 = ['x1', 'x2'], (lambda input_samples: -input_samples[0] * 2*input_samples[1] + input_samples[2])\n",
    "ik_x4, se_x4 = ['x1', 'x3'], (lambda input_samples: np.log(1+np.abs(input_samples[0])) + input_samples[1] + input_samples[2])\n",
    "\n",
    "ik_y, se_y = ['h','x1','x2','x3','x4'], (lambda input_samples:\n",
    "                                        2*input_samples[0] \n",
    "                                         + input_samples[1]\n",
    "                                        - input_samples[2]\n",
    "                                         - (input_samples[2]>0)*(input_samples[3]<-1)*5.\n",
    "                                         + (input_samples[1]<0)*(input_samples[4]<-1)*2.\n",
    "                                         + (input_samples[1]<0)*(input_samples[2]<1)*(input_samples[3]<-1)*3.\n",
    "                                         + 0.5*input_samples[5])\n",
    "ik_x5, se_x5 = ['x2', 'x4', 'y'], (lambda input_samples: 2*input_samples[0] + input_samples[1] - input_samples[2] + input_samples[3])\n",
    "\n",
    "fh = {'input_keys':ik_h, 'structural_eq': se_h}\n",
    "f1 = {'input_keys':ik_x1, 'structural_eq': se_x1}\n",
    "f2 = {'input_keys':ik_x2, 'structural_eq': se_x2}\n",
    "f3 = {'input_keys':ik_x3, 'structural_eq': se_x3}\n",
    "f4 = {'input_keys':ik_x4, 'structural_eq': se_x4}\n",
    "f5 = {'input_keys':ik_x5, 'structural_eq': se_x5}\n",
    "fy = {'input_keys':ik_y, 'structural_eq': se_y}\n",
    "\n",
    "sh = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s1 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s2 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s3 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s4 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s5 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "sy = lambda n_samples: np.random.normal(size=n_samples)\n",
    "\n",
    "topo_order = ['h','x1','x2','x3','x4','y','x5']\n",
    "y_key = 'y'\n",
    "x_key = ['x1','x2','x3','x4','x5']\n",
    "\n",
    "structural_equation_dict = {'x1':f1, 'x2':f2, 'x3':f3, 'y':fy, 'x4':f4, 'x5':f5, 'h':fh}\n",
    "disturbance_sampler_dict = {'x1':s1, 'x2':s2, 'x3':s3, 'y':sy, 'x4':s4, 'x5':s5, 'h':sh}\n",
    "\n",
    "base = BaseEnvironment(structural_equation_dict, disturbance_sampler_dict, topo_order, y_key, x_key)\n",
    "coll_env = CollectionEnvironment(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples_list = [100,200,500,1000,2000,5000,10000]\n",
    "loss_mean_list, loss_std_list, naive_loss_mean_list, naive_loss_std_list = [], [], [], []\n",
    "\n",
    "for n_samples in n_samples_list:\n",
    "    loss_list=[]\n",
    "    naive_loss_list=[]\n",
    "    n_rep=10\n",
    "    for i in np.arange(n_rep):\n",
    "        coll_env.reset_env()\n",
    "        coll_env.add_env('e1', {'x1':{'type':'independent'},'x2':{'type':'independent'},'x3':{'type':'independent'}}, n_samples)\n",
    "        coll_env.add_env('e2', {'x1':{'type':'independent'},'x4':{'type':'independent'}}, n_samples)\n",
    "        coll_env.add_env('e3', {'x2':{'type':'independent'},'x3':{'type':'independent'}}, n_samples)\n",
    "        \n",
    "        backfit = Backfitting(DecisionTreeRegression, \n",
    "                              'boosting',\n",
    "                              max_n_iter=10,\n",
    "                              gap_convergence=1e-5,\n",
    "                              warm_start=False, \n",
    "                              params_method={},\n",
    "                              reweighting_candidates=True,\n",
    "                              update_within_loop=False\n",
    "                             )\n",
    "        try:\n",
    "            loss_list.append(backfit.fitCV(coll_env))\n",
    "            naive_loss_list.append(backfit.fit_naiveCV(coll_env))   \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    loss_mean_list.append(np.mean(np.stack(loss_list)[:,1]))\n",
    "    loss_std_list.append(np.std(np.stack(loss_list)[:,1]))\n",
    "    naive_loss_mean_list.append(np.mean(np.stack(naive_loss_list)[:,1]))\n",
    "    naive_loss_std_list.append(np.std(np.stack(naive_loss_list)[:,1]))\n",
    "print(loss_mean_list, loss_std_list, naive_loss_mean_list, naive_loss_std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misspecified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ik_h, se_h = [], (lambda input_samples:input_samples)\n",
    "ik_x1, se_x1 = ['h'], (lambda input_samples: 2*input_samples[0] + input_samples[1])\n",
    "ik_x2, se_x2 = ['h', 'x1'], (lambda input_samples: input_samples[0] + input_samples[1] + input_samples[2])\n",
    "ik_x3, se_x3 = ['x1', 'x2'], (lambda input_samples: -input_samples[0] * 2*input_samples[1] + input_samples[2])\n",
    "ik_x4, se_x4 = ['x1', 'x3'], (lambda input_samples: np.log(1+np.abs(input_samples[0])) + input_samples[1] + input_samples[2])\n",
    "\n",
    "ik_y, se_y = ['h','x1','x2','x3','x4'], (lambda input_samples:\n",
    "                                         0.3*input_samples[0] \n",
    "                                         + 1*input_samples[1]**2\n",
    "                                         #- 0.5*input_samples[3]*input_samples[4]\n",
    "                                         - 1*input_samples[2]*input_samples[4] + 0.3*input_samples[5])\n",
    "ik_x5, se_x5 = ['x2', 'x4', 'y'], (lambda input_samples: 2*input_samples[0] + input_samples[1] - input_samples[2] + input_samples[3])\n",
    "\n",
    "fh = {'input_keys':ik_h, 'structural_eq': se_h}\n",
    "f1 = {'input_keys':ik_x1, 'structural_eq': se_x1}\n",
    "f2 = {'input_keys':ik_x2, 'structural_eq': se_x2}\n",
    "f3 = {'input_keys':ik_x3, 'structural_eq': se_x3}\n",
    "f4 = {'input_keys':ik_x4, 'structural_eq': se_x4}\n",
    "f5 = {'input_keys':ik_x5, 'structural_eq': se_x5}\n",
    "fy = {'input_keys':ik_y, 'structural_eq': se_y}\n",
    "\n",
    "sh = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s1 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s2 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s3 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s4 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "s5 = lambda n_samples: np.random.normal(size=n_samples)\n",
    "sy = lambda n_samples: np.random.normal(size=n_samples)\n",
    "\n",
    "topo_order = ['h','x1','x2','x3','x4','y','x5']\n",
    "y_key = 'y'\n",
    "x_key = ['x1','x2','x3','x4','x5']\n",
    "\n",
    "structural_equation_dict = {'x1':f1, 'x2':f2, 'x3':f3, 'y':fy, 'x4':f4, 'x5':f5, 'h':fh}\n",
    "disturbance_sampler_dict = {'x1':s1, 'x2':s2, 'x3':s3, 'y':sy, 'x4':s4, 'x5':s5, 'h':sh}\n",
    "\n",
    "base = BaseEnvironment(structural_equation_dict, disturbance_sampler_dict, topo_order, y_key, x_key)\n",
    "coll_env = CollectionEnvironment(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples_list = [100,200,500,1000,5000,10000,50000,100000]\n",
    "loss_mean_list, loss_std_list, naive_loss_mean_list, naive_loss_std_list = [], [], [], []\n",
    "\n",
    "for n_samples in n_samples_list:\n",
    "    loss_list=[]\n",
    "    naive_loss_list=[]\n",
    "    n_rep=10\n",
    "    for i in np.arange(n_rep):\n",
    "        coll_env.reset_env()\n",
    "        coll_env.add_env('e1', {'x1':{'type':'independent'},'x2':{'type':'independent'},'x3':{'type':'independent'}}, n_samples)\n",
    "        coll_env.add_env('e2', {'x1':{'type':'independent'},'x4':{'type':'independent'}}, n_samples)\n",
    "        coll_env.add_env('e3', {'x2':{'type':'independent'},'x3':{'type':'independent'}}, n_samples)\n",
    "        \n",
    "        backfit = Backfitting(DecisionTreeRegression, \n",
    "                              'boosting',\n",
    "                              max_n_iter=10,\n",
    "                              gap_convergence=1e-5,\n",
    "                              warm_start=False, \n",
    "                              params_method={},\n",
    "                              reweighting_candidates=True,\n",
    "                              update_within_loop=False\n",
    "                             )\n",
    "        try:\n",
    "            loss_list.append(backfit.fitCV(coll_env, range_min=-3, range_max=2, range_step =0.2, print_selected=True))\n",
    "            naive_loss_list.append(backfit.fit_naiveCV(coll_env))   \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    loss_mean_list.append(np.mean(np.stack(loss_list)[:,1]))\n",
    "    loss_std_list.append(np.std(np.stack(loss_list)[:,1]))\n",
    "    naive_loss_mean_list.append(np.mean(np.stack(naive_loss_list)[:,1]))\n",
    "    naive_loss_std_list.append(np.std(np.stack(naive_loss_list)[:,1]))\n",
    "print(loss_mean_list, loss_std_list, naive_loss_mean_list, naive_loss_std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_mean_list, loss_std_list, naive_loss_mean_list, naive_loss_std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
